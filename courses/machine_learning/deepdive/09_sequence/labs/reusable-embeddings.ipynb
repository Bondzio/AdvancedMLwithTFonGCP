{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using pre-trained embeddings with TensorFlow Hub</h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "    <li>How to instantiate a TensorFlow Hub module</li>\n",
    "    <li>How to find pre-trained TensorFlow Hub modules for a variety of purposes</li>\n",
    "    <li>How to examine the embeddings of a Hub module</li>\n",
    "    <li>How one Hub module composes representations of sentences from individual words</li>\n",
    "    <li>How to assess word embeddings using a semantic similarity test</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the TensorFlow Hub library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub==0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 4.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting numpy>=1.12.0 (from tensorflow-hub==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/21/2b18339d24a2f73dcefb2f10f48aff6182e16da83e3a612684443c6cfb29/numpy-1.17.2-cp35-cp35m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 72kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.10.0 (from tensorflow-hub==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.4.0 (from tensorflow-hub==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9c/3e/05df91cc2d08eec88b1869962beb6b144755e7d4a7a6668e1b9f0f450037/protobuf-3.9.2-cp35-cp35m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from protobuf>=3.4.0->tensorflow-hub==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl (576kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy, six, setuptools, protobuf, tensorflow-hub\n",
      "Successfully installed numpy-1.17.2 protobuf-3.9.2 setuptools-41.2.0 six-1.12.0 tensorflow-hub-0.4.0\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 15kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting numpy>=1.13.3 (from tensorflow==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/21/2b18339d24a2f73dcefb2f10f48aff6182e16da83e3a612684443c6cfb29/numpy-1.17.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 4.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 466kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 11.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/3a/3df7d5a06d65f8d3064bb03b0e11e3092d125f933eb11ab658a4d589b92c/grpcio-1.23.0-cp35-cp35m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 697kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/9c/3e/05df91cc2d08eec88b1869962beb6b144755e7d4a7a6668e1b9f0f450037/protobuf-3.9.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/0d/7cbf64cac3f93617a2b6b079c0182e4a83a3e7a8964d3b0cc3d9758ba002/absl-py-0.8.0.tar.gz (102kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 12.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/56/d5c53cd170529bb40cd7dd43e2b68944cb65a45f65ab4c78a68f4ac9e51e/h5py-2.10.0-cp35-cp35m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 531kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 12.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from protobuf>=3.6.1->tensorflow==1.13.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: termcolor, gast, absl-py\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/9a/1e/7a/456008eb5e47fd5de792c6139df6d5b3d5f71d51c6a0b94799\n",
      "Successfully built termcolor gast absl-py\n",
      "Installing collected packages: termcolor, numpy, six, h5py, keras-applications, absl-py, mock, tensorflow-estimator, wheel, setuptools, protobuf, markdown, werkzeug, grpcio, tensorboard, keras-preprocessing, gast, astor, tensorflow\n",
      "Successfully installed absl-py-0.8.0 astor-0.8.0 gast-0.3.2 grpcio-1.23.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.17.2 protobuf-3.9.2 setuptools-41.2.0 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-hub==0.4.0\n",
    "!pip3 install --upgrade tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to restart your notebook's kernel before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TensorFlow Hub Concepts</h2>\n",
    "\n",
    "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning, which we covered as part of the course on Image Models.\n",
    "\n",
    "To download and use a module, it's as easy as:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  module_url = \"path/to/hub/module\"\n",
    "  embed = hub.Module(module_url)\n",
    "  embeddings = embed([\"word1\", \"word2\", \"word3\"])\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because modules are self-contained parts of a TensorFlow graph, in order to actually collect values from a module, you'll need to evaluate it in the context of a session."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  # .... earlier code\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore what hub modules there are. Go to [the documentation page](https://www.tensorflow.org/hub/modules) and explore a bit.\n",
    "\n",
    "Note that TensorFlow Hub has modules for Images, Text, and Other. In this case, we're interested in a Text module, so navigate to the Text section.\n",
    "\n",
    "Within the Text section, there are a number of modules. If you click on a link, you'll be taken to a page that describes the module and links to the original paper where the model was proposed. Click on a model in the Word2Vec section of the page.\n",
    "\n",
    "Note the details section, which describes what the module expects as input, how it preprocesses data, what it does when it encounters a word it hasn't seen before (OOV means \"out of vocabulary\") and in this case, how word embeddings can be composed to form sentence embeddings.\n",
    "\n",
    "Finally, note the URL of the page. This is the URL you can copy to instantiate your module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 1: Create an embedding using the NNLM model</h2>\n",
    "\n",
    "To complete this task:\n",
    "<ol>\n",
    "    <li>Find the module URL for the NNLM 50 dimensional English model</li>\n",
    "    <li>Use it to instantiate a module as 'embed'</li>\n",
    "    <li>Print the embedded representation of \"cat\"</li>\n",
    "</ol>\n",
    "\n",
    "NOTE: downloading hub modules requires downloading a lot of data. Instantiating the module will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "embed = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I completed this exercise, I got a vector that looked like:\n",
    "[[ 0.11233182 -0.3176392  -0.01661182...]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 2: Assess the Embeddings Informally</h2>\n",
    "\n",
    "<ol>\n",
    "    <li>Identify some words to test</li>\n",
    "    <li>Retrieve the embeddings for each word</li>\n",
    "    <li>Determine what method to use to compare each pair of embeddings</li>\n",
    "</ol>    \n",
    "\n",
    "So, now we have some vectors but the question is, are they any good? One way of testing whether they are any good is to try them for your task. But, first, let's just take a peak. \n",
    "\n",
    "For our test, we'll need three common words such that two of the words are much closer in meaning than the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = #\n",
    "word_2 = #\n",
    "word_3 = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the same process of using our Hub module to generate embeddings but instead of printing the embeddings, capture them in a variable called 'my_embeddings'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use Seaborn's heatmap function to see how the vectors compare to each other. I've written the shell of a function that you'll need to complete that will generate a heatmap. The one piece that's missing is how we'll compare each pair of vectors. Note that because we are computing a score for every pair of vectors, we should have len(my_embeddings)^2 scores. There are many valid ways of comparing vectors. Generality, similarity scores are symmetric. The simplest is to take their dot product. For extra credit, implement a more complicated vector comparison function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, embeddings):\n",
    "  corr = # ... TODO: fill out a len(embeddings) x len(embeddings) array\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=90)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "plot_similarity([word_1, word_2, word_3], my_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should observe is that, trivially, all words are identical to themselves, and, more interestingly, that the two more similar words have more similar embeddings than the third word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 3: From Words to Sentences</h2>\n",
    "\n",
    "Up until now, we've used our module to produce representations of words. But, in fact, if we want to, we can also use it to construct representations of sentences. The methods used by the module to compose a representation of a sentence won't be as nuanced as what an RNN might do, but they are still worth examining because they are so convenient.\n",
    "\n",
    "<ol>\n",
    "    <li> Examine the documentation for our hub module and determine how to ask it to construct a representation of a sentence</li>\n",
    "    <li> Figure out how the module takes word embeddings and uses them to construct sentence embeddings </li>\n",
    "    <li> Construct a embeddings of a \"cat\", \"The cat sat on the mat\", \"dog\" and \"The cat sat on the dog\"  and plot their similarity\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is cat more similar to, \"The cat sat on the mat\" or \"dog\"? Is this desireable?\n",
    "\n",
    "Think back to how an RNN scans a sequence and maintains its state. Naive methods of embedding composition (mapping many to one) can't possibly compete with a network trained for this very purpose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 4: Assessing the Embeddings Formally</h2>\n",
    "Of course, it's great to know that our embeddings match our intuitions to an extent, but it'd be better to have a formal, data-driven measure of the quality of the representation.\n",
    "\n",
    "Researchers have\n",
    "The [STS Benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. Pearson correlation is then used to evaluate the quality of the machine similarity scores against human judgements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sts_dataset(filename):\n",
    "  # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
    "  # sentences and their human rated similarity score.\n",
    "  sent_pairs = []\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      ts = line.strip().split(\"\\t\")\n",
    "      # (sent_1, sent_2, similarity_score)\n",
    "      sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "  return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def download_and_load_sts_data():\n",
    "  sts_dataset = tf.keras.utils.get_file(\n",
    "      fname=\"Stsbenchmark.tar.gz\",\n",
    "      origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
    "      extract=True)\n",
    "\n",
    "  sts_dev = load_sts_dataset(\n",
    "      os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
    "  sts_test = load_sts_dataset(\n",
    "      os.path.join(\n",
    "          os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
    "\n",
    "  return sts_dev, sts_test\n",
    "\n",
    "\n",
    "sts_dev, sts_test = download_and_load_sts_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man with a hard hat is dancing.</td>\n",
       "      <td>A man wearing a hard hat is dancing.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A young child is riding a horse.</td>\n",
       "      <td>A child is riding a horse.</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is feeding a mouse to a snake.</td>\n",
       "      <td>The man is feeding a mouse to the snake.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A woman is playing the guitar.</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A woman is playing the flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sent_1  \\\n",
       "0     A man with a hard hat is dancing.   \n",
       "1      A young child is riding a horse.   \n",
       "2  A man is feeding a mouse to a snake.   \n",
       "3        A woman is playing the guitar.   \n",
       "4         A woman is playing the flute.   \n",
       "\n",
       "                                     sent_2   sim  \n",
       "0      A man wearing a hard hat is dancing.  5.00  \n",
       "1                A child is riding a horse.  4.75  \n",
       "2  The man is feeding a mouse to the snake.  5.00  \n",
       "3                  A man is playing guitar.  2.40  \n",
       "4                 A man is playing a flute.  2.75  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build the Evaluation Graph</h3>\n",
    "\n",
    "Next, we need to build the evaluation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "# For evaluation we use exactly normalized rather than\n",
    "# approximately normalized.\n",
    "sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\n",
    "sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\n",
    "cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
    "sim_scores = 1.0 - tf.acos(clip_cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate Sentence Embeddings</h3>\n",
    "\n",
    "Finally, we need to create a session and run our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = sts_data['sent_1'].tolist()\n",
    "text_b = sts_data['sent_2'].tolist()\n",
    "dev_scores = sts_data['sim'].tolist()\n",
    "\n",
    "def run_sts_benchmark(session):\n",
    "  \"\"\"Returns the similarity scores\"\"\"\n",
    "  emba, embb, scores = session.run(\n",
    "      [sts_encode1, sts_encode2, sim_scores],\n",
    "      feed_dict={\n",
    "          sts_input1: text_a,\n",
    "          sts_input2: text_b\n",
    "      })\n",
    "  return scores\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  scores = run_sts_benchmark(session)\n",
    "\n",
    "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
    "    pearson_correlation[0], pearson_correlation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Credit</h3>\n",
    "\n",
    "For extra credit, re-run this analysis with a different Hub module. Are the results different? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Further Reading</h2>\n",
    "\n",
    "We published a [blog post](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html) on how bias can affect text embeddings. It's worth a read!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
